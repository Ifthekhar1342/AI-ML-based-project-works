# -*- coding: utf-8 -*-
"""Multimodal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t-8Ycwsxuvvy58wXj9Yej-GX60Eyc4Pw
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Libaries"""

# Importing all the necessary libraries
import keras
import h5py
from keras import optimizers
from keras.models import load_model
from keras.layers import Bidirectional
from keras.layers.core import Reshape, Dropout
from keras.utils.vis_utils import plot_model
import os
# import keras_metrics
import matplotlib.pyplot as plt
from keras.layers import Conv1D,Dense, MaxPooling1D, Flatten, GlobalAveragePooling2D,GlobalAveragePooling3D
from keras import regularizers
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from keras import regularizers
from keras.applications.inception_v3 import InceptionV3
from tensorflow.keras import models
from tensorflow.keras import layers
import pandas as pd
from sklearn.preprocessing import LabelEncoder
import re
from nltk.corpus import stopwords
from nltk import word_tokenize
from keras.preprocessing import image
from PIL import Image, ImageFile
from keras import preprocessing, Input
import numpy as np
#model
from keras.applications.vgg16 import VGG16
from keras.applications.vgg19 import VGG19
from keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.applications.resnet50 import ResNet50
from keras.models import Model
from keras.callbacks import ModelCheckpoint, EarlyStopping
from sklearn.metrics import classification_report,f1_score
import pickle as pkl

# Keras Functional API

import tensorflow as tf
from tensorflow import keras
from keras.layers import Input, Dense, Activation, Dropout,Flatten,Embedding
from keras.layers import Conv1D,MaxPooling1D,GlobalAveragePooling1D
from keras.models import Model
from tensorflow.keras.optimizers import Adam,SGD,Nadam,RMSprop
from tensorflow.keras.models import load_model

# Keras Functional API

import tensorflow as tf
from tensorflow import keras
from keras.layers import Input, Dense, Activation, Dropout,Flatten,Embedding
from keras.layers import Conv1D,MaxPooling1D,GlobalAveragePooling1D, Bidirectional, LSTM, GRU
from keras.models import Model
from tensorflow.keras.models import load_model

dataset_path = "/content/drive/MyDrive/DATASET/"
# Validation_path = "/content/drive/MyDrive/DATASET/val_data.csv"
img_dir = "/content/drive/MyDrive/multimodal_image"

"""## Fetching Data"""

train_data = pd.read_excel(dataset_path+'train.xlsx')
test_data = pd.read_csv(dataset_path+'test.xlsx')
print("training size:", len(train_data))
print("testing size:", len(test_data))

train_data.head()

train_data.columns

train_data['label'].value_counts()

# Encode Labels
train_data["enc_label"]=train_data.label.replace({"non_damage":0,"damaged_infrastructure":1,"damaged_nature":2,"flood":3,"fires":4,"human_damage":5})
test_data["enc_label"]=test_data.label.replace({"non_damage":0,"damaged_infrastructure":1,"damaged_nature":2,"flood":3,"fires":4,"human_damage":5})

# convert float type data into str if any occurs
train_data["tweet"]=train_data["tweet"].astype(str)
test_data["tweet"]=test_data["tweet"].astype(str)

train_data['enc_label'].value_counts()

def replace_string(row):
  return row.replace('.JPG','.jpg')

train_data['image'] = train_data['image'].apply(replace_string)
test_data['image'] = test_data['image'].apply(replace_string)

"""## Reading Image"""

def create_img_path(DF, Col_name, img_dir):
    img_path = [img_dir + '/' + name for name in DF[Col_name]]
    return img_path

# Creating train, test and validation image path
train_img_path = create_img_path(train_data,'image', img_dir)
test_img_path = create_img_path(test_data,'image', img_dir)

train_img_path[0]

"""#Image Fetching"""

def get_input(path):
    ImageFile.LOAD_TRUNCATED_IMAGES = True
    img = Keras.utilis.load_img(path,target_size = (100,100))
    return(img)

# Takes in image and preprocess it
def process_input(img):
    # Converting image to array
    img_data = keras.utils.img_to_array(img)
    # Adding one more dimension to array
    img_data = np.expand_dims(img_data, axis=0)
    #
    # img_data = preprocess_input(img_data)
    return(img_data)

from tensorflow.keras.preprocessing.image import load_img
    from tensorflow.keras.preprocessing.image import img_to_array
    from tensorflow.keras.applications.vgg16 import preprocess_input
    from tensorflow.keras.applications.vgg16 import decode_predictions
    from tensorflow.keras.applications.vgg16 import VGG16

# Create an array of training images
train_images =[]
for n,i in enumerate(train_img_path):
      input_img = get_input(i)
      process_img = process_input(input_img)
      print(n)
      train_images.append(process_img[0])

#  #Create an array of test images
 test_images = []
 for n,i in enumerate(test_img_path):
 input_img = get_input(i)
  process_img = process_input(input_img)
  print(n)
   test_images.append(process_img[0])

#  convert into numpy array
 train_images  =  np.array(train_images)
# # convert into numpy array
 test_image = np.array(test_images)

with open(dataset_path+'train.pkl','wb') as f:
    pkl.dump(train_image, f)

with open(dataset_path+'test.pkl','wb') as f:
    pkl.dump(test_image, f)

"""#Load Images"""

with open(dataset_path+'train.pkl','rb') as f:
  train_image = pkl.load(f)
  print("Training Images:-- ",train_image.shape)

with open(dataset_path+'test.pkl','rb') as f:
  test_image = pkl.load(f)
  print("Test Images:-- ",test_image.shape)

"""## Callback"""

def checkpoint_fn(name):
  filepath =f'/content/drive/MyDrive/New folder/Model/{name}.h5'
  checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')
  early = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=20, verbose=1, mode='auto')
  return [checkpoint,early]

"""## VGG16 model"""

def VGG16_model(input_shape, n_classes, optimizer, fine_tune=0):
    """
    Compiles a model integrated with VGG16 pretrained layers

    input_shape: tuple - the shape of input images (width, height, channels)
    n_classes: int - number of classes for the output layer
    optimizer: string - instantiated optimizer to use for training. Defaults to 'RMSProp'
    fine_tune: int - The number of pre-trained layers to unfreeze.
                If set to 0, all pretrained layers will freeze during training
    """

    # Pretrained convolutional layers are loaded using the Imagenet weights.
    # Include_top is set to False, in order to exclude the model's fully-connected layers.
    conv_base = VGG16(include_top=False,
                     weights='imagenet',
                     input_shape=input_shape)

    # Defines how many layers to freeze during training.
    # Layers in the convolutional base are switched from trainable to non-trainable
    # depending on the size of the fine-tuning parameter.
    if fine_tune > 0:
        for layer in conv_base.layers[:-fine_tune]:
            layer.trainable = False
    else:
        for layer in conv_base.layers:
            layer.trainable = False

    # Create a new 'top' of the model (i.e. fully-connected layers).
    # This is 'bootstrapping' a new top_model onto the pretrained layers.
    top_model = conv_base.output
    top_model = Flatten()(top_model)
    # top_model = Dense(4096, activation='relu')(top_model)
    top_model = Dense(536, activation='relu')(top_model)
    # top_model = Dropout(0.2)(top_model)
    output_layer = Dense(n_classes, activation='softmax')(top_model)

    # Group the convolutional base and new fully-connected layers into a Model object.
    model = Model(inputs=conv_base.input, outputs=output_layer)

    # Compiles the model for training.
    model.compile(optimizer=optimizer,
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    return model

input_shape = (100, 100, 3)
n_epochs = 30
n_classes = 6

# Building img_prediction layer using Image_model
vgg16_img = VGG16_model(input_shape, n_classes,Adam(3e-3), fine_tune=2)

# base_img.summary()

vgg16_history = vgg16_img.fit(train_image,train_data['enc_label'],
                            batch_size=32,
                            epochs=n_epochs,
                            callbacks=checkpoint_fn('vgg16'),
                            validation_split = 0.1)

from keras.models import load_model
vgg16_model = load_model('/content/drive/MyDrive/New folder/Model/vgg16.h5')

y_pred=np.argmax(vgg16_model.predict(test_image),axis=-1)

print(classification_report(test_data["enc_label"],y_pred))

f1_score(test_data["enc_label"], y_pred, average='weighted')

"""# Build VGG19 model"""

# from keras.applications.vgg16 import preprocess_input

"""## VGG19 Model"""

def VGG19_model(input_shape, n_classes, optimizer='rmsprop'):
    """
    Compiles a model integrated with VGG16 pretrained layers

    input_shape: tuple - the shape of input images (width, height, channels)
    n_classes: int - number of classes for the output layer
    optimizer: string - instantiated optimizer to use for training. Defaults to 'RMSProp'
    fine_tune: int - The number of pre-trained layers to unfreeze.
                If set to 0, all pretrained layers will freeze during training
    """
    keras.backend.clear_session()
    # Pretrained convolutional layers are loaded using the Imagenet weights.
    # Include_top is set to False, in order to exclude the model's fully-connected layers.
    conv_base = VGG19(include_top=False,
                     weights='imagenet',
                     input_shape=input_shape)

    # Defines how many layers to freeze during training.
    # Layers in the convolutional base are switched from trainable to non-trainable
    # depending on the size of the fine-tuning parameter.

    for layer in conv_base.layers:
          layer.trainable = False

    # Create a new 'top' of the model (i.e. fully-connected layers).
    # This is 'bootstrapping' a new top_model onto the pretrained layers.
    top_model = conv_base.output
    top_model = Flatten()(top_model)
    # top_model = Dense(4096, activation='relu')(top_model)
    top_model = Dense(536, activation='relu')(top_model)
    # top_model = Dropout(0.2)(top_model)
    output_layer = Dense(n_classes, activation='softmax')(top_model)

    # Group the convolutional base and new fully-connected layers into a Model object.
    model = Model(inputs=conv_base.input, outputs=output_layer)

    # Compiles the model for training.
    model.compile(optimizer=optimizer,
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    return model

input_shape = (100, 100, 3)
n_epochs = 30
n_classes = 6

# Building img_prediction layer using Image_model
vgg19_img = VGG19_model(input_shape, n_classes,Adam(learning_rate=1e-3))

vgg19_history = vgg19_img.fit(train_image,train_data['enc_label'],
                            batch_size=32,
                           epochs=n_epochs,
                           callbacks=checkpoint_fn('vgg19'),
                            validation_split= 0.1 )

from keras.models import load_model
saved_model = load_model('/content/drive/MyDrive/New folder/Model/vgg19.h5')

y_pred=np.argmax(saved_model.predict(test_image),axis=-1)

print(classification_report(test_data["enc_label"],y_pred))

f1_score(test_data["enc_label"], y_pred, average='weighted')

"""# Build Inception Model"""

# from keras.applications.inception_v3 import preprocess_input

"""## Inception Model"""

def INCEPTION_model(input_shape, n_classes, optimizer='rmsprop'):
    """
    Compiles a model integrated with VGG16 pretrained layers

    input_shape: tuple - the shape of input images (width, height, channels)
    n_classes: int - number of classes for the output layer
    optimizer: string - instantiated optimizer to use for training. Defaults to 'RMSProp'
    fine_tune: int - The number of pre-trained layers to unfreeze.
                If set to 0, all pretrained layers will freeze during training
    """
    keras.backend.clear_session()
    # Pretrained convolutional layers are loaded using the Imagenet weights.
    # Include_top is set to False, in order to exclude the model's fully-connected layers.
    conv_base = InceptionV3(include_top=False,
                     weights='imagenet',
                     input_shape=input_shape)

    # Defines how many layers to freeze during training.
    # Layers in the convolutional base are switched from trainable to non-trainable
    # depending on the size of the fine-tuning parameter.

    for layer in conv_base.layers:
          layer.trainable = False

    # Create a new 'top' of the model (i.e. fully-connected layers).
    # This is 'bootstrapping' a new top_model onto the pretrained layers.
    top_model = conv_base.output
    top_model = Flatten()(top_model)
    # top_model = Dense(4096, activation='relu')(top_model)
    top_model = Dense(536, activation='relu')(top_model)
    # top_model = Dropout(0.2)(top_model)
    output_layer = Dense(n_classes, activation='softmax')(top_model)

    # Group the convolutional base and new fully-connected layers into a Model object.
    model = Model(inputs=conv_base.input, outputs=output_layer)

    # Compiles the model for training.
    model.compile(optimizer=optimizer,
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    return model

input_shape = (100, 100, 3)
n_epochs = 30
n_classes = 6

# Building img_prediction layer using Image_model
inception_img = INCEPTION_model(input_shape, n_classes,Adam(learning_rate=1e-3))

incept_history = inception_img.fit(train_image,train_data['enc_label'],
                            batch_size=32,
                           epochs=n_epochs,
                           callbacks=checkpoint_fn('Inception'),
                            validation_split = 0.1)

from keras.models import load_model
saved_model = load_model('/content/drive/MyDrive/New folder/Model/Inception.h5')

y_pred=np.argmax(saved_model.predict(test_image),axis=-1)

print(classification_report(test_data["enc_label"],y_pred))

f1_score(test_data["enc_label"], y_pred, average='weighted')

"""# Build Resnet Model"""

# from tensorflow.keras.applications.resnet50  import preprocess_input

"""## Resnet50"""

def Resnet50_model(input_shape, n_classes, optimizer='rmsprop'):
    """
    Compiles a model integrated with VGG16 pretrained layers

    input_shape: tuple - the shape of input images (width, height, channels)
    n_classes: int - number of classes for the output layer
    optimizer: string - instantiated optimizer to use for training. Defaults to 'RMSProp'
    fine_tune: int - The number of pre-trained layers to unfreeze.
                If set to 0, all pretrained layers will freeze during training
    """

    # Pretrained convolutional layers are loaded using the Imagenet weights.
    # Include_top is set to False, in order to exclude the model's fully-connected layers.
    keras.backend.clear_session()
    conv_base = ResNet50(include_top=False,
                     weights='imagenet',
                     input_shape=input_shape)

    # Defines how many layers to freeze during training.
    # Layers in the convolutional base are switched from trainable to non-trainable
    # depending on the size of the fine-tuning parameter.
    for layer in conv_base.layers:
        layer.trainable = False

    # Create a new 'top' of the model (i.e. fully-connected layers).
    # This is 'bootstrapping' a new top_model onto the pretrained layers.
    top_model = conv_base.output
    top_model = GlobalAveragePooling2D()(top_model)
    # # top_model = Dense(4096, activation='relu')(top_model)
    top_model = Dense(100, activation='relu')(top_model)
    # top_model = Dropout(0.2)(top_model)
    output_layer = Dense(n_classes, activation='softmax')(top_model)

    # Group the convolutional base and new fully-connected layers into a Model object.
    model = Model(inputs=conv_base.input, outputs=output_layer)

    # Compiles the model for training.
    model.compile(optimizer=optimizer,
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    return model

input_shape = (100, 100, 3)
n_epochs = 30
n_classes = 6

# Building img_prediction layer using Image_model
resnet_img = Resnet50_model(input_shape, n_classes, Adam(learning_rate=1e-3))

resnet_history = resnet_img.fit(train_image,train_data['enc_label'],
                            batch_size=32,
                           epochs=n_epochs,
                           callbacks=checkpoint_fn('Resnet50'),
                            validation_split=0.1)

from keras.models import load_model
saved_model = load_model('/content/drive/MyDrive/New folder/Model/Resnet50.h5')

y_pred=np.argmax(saved_model.predict(test_image),axis=-1)

print(classification_report(test_data["enc_label"],y_pred))

f1_score(test_data["enc_label"], y_pred, average='weighted')

"""# Text Model

## Clean Text
"""

import nltk
nltk.download('stopwords')

STOPWORDS = set(stopwords.words('english'))
REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
EMAIL = re.compile('^([a-zA-Z0-9_\-\.]+)@([a-zA-Z0-9_\-\.]+)\.([a-zA-Z]{2,5})$')
# NUMBERS = re.compile(['0-9'])
STOPWORDS = set(stopwords.words('english'))

# def clean_text(text):
#     """
#         text: a string

#         return: modified initial string
#     """
#     text = text.lower()
#     text = EMAIL.sub('', text)
# # #     text = NUMBERS.sub('',text)
# #     text = REPLACE_BY_SPACE_RE.sub(' ',text)
# #     text = BAD_SYMBOLS_RE.sub('',text)
# #     text = text.replace('x','')
#     # text = ' '.join(word for word in text.split() if word not in STOPWORDS)

#     return text

'''
Text Cleaning
'''
from bs4 import BeautifulSoup
def clean_text(row):
  #to remove HTML tags
  text = BeautifulSoup(row, 'html.parser').get_text()
  d = re.sub(r'(https|http)?:\/\/(\w|\.|\/|\?|\=|\&|\%)*\b', '', text, flags=re.MULTILINE) #This line is for removing url
  post = d.replace('\n', '')
  post = post.replace('â€”', ' ')
  # to remove accented characters
  # to remove special characters and numbers
  # define the pattern to keep
  pat = r'[^#@a-zA-z0-9]'
  text = re.sub(pat, ' ', post)
  #to remove punctuation
  #text = ''.join([c for c in text if c not in string.punctuation])
  # to remove special characters
  #pattern = r'^\s*|\s\s*'
  #text = re.sub(pattern, ' ', text).strip()
  # convert into lower case
  text = text.lower()
  # Stopword Removing
  #tokenizer = ToktokTokenizer()
  # convert sentence into token of words
  #tokens = tokenizer.tokenize(text)
  #tokens = [token.strip() for token in tokens]


  return text

train_data['clean_Text'] = train_data['tweet'].apply(clean_text)
test_data['clean_Text'] = test_data['tweet'].apply(clean_text)

test_data['clean_Text'][0]

np.unique(train_data['enc_label'])

train_data['enc_label'].value_counts()

test_data.head()

"""## Tokenization"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# max_words = 100000
# 
# from keras.preprocessing.text import Tokenizer
# 
# tokenizer = Tokenizer(num_words = max_words, filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n-',
#                       split=' ', char_level=False, oov_token=None, document_count=0)
# 
# tokenizer.fit_on_texts(train_data['tweet'])
#

# Commented out IPython magic to ensure Python compatibility.
# %%time
# word_counts = tokenizer.word_counts
# word_docs = tokenizer.word_docs
# word_index = tokenizer.word_index
# document_count = tokenizer.document_count
# vocab_size = len(word_counts) + 1
# print(vocab_size)

"""# Encoding Data into Numbers"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Convert string into list of integer indices
# train_sequences = tokenizer.texts_to_sequences(train_data['tweet'])
# test_sequences = tokenizer.texts_to_sequences(test_data['tweet'])

word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))
# print(train_data['cleanText'])
print(train_sequences[4663])

"""### Pad sequence"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# ## Before padding length of different training examples
# # train data
# mx = len(train_sequences[0])
# for x in train_sequences:
#   mx =max(mx, len(x))
# print("Maximum Text length",mx)
# #test data
# mx = len(test_sequences[0])
# for x in test_sequences:
#   mx =max(mx, len(x))
# print("Maximum Text length",mx)
# #validation data
# mx = len(test_sequences[0])
# for x in test_sequences:
#   mx =max(mx, len(x))
# print("Maximum Text length",mx)

from keras_preprocessing.sequence import pad_sequences

##We can also determine maxlen by plotting the frequency distribution of the lengths
train_corpus = pad_sequences(train_sequences, value=0.0, padding='post', maxlen= 100)
test_corpus = pad_sequences(test_sequences, value=0.0, padding='post', maxlen= 100)

## We get the maxlen value from the Length frequency distribution

print(train_corpus[10])
print(test_corpus[1])

y_train=np.array(train_data['enc_label']).reshape(-1,1)
y_test=np.array(test_data['enc_label']).reshape(-1,1)

"""## CNN with keras embedding

## Model Parameters
"""

max_length = 100
embedding_dim = 100
number_of_classes = 6

keras.backend.clear_session()

# define CNN model

def CNN():

  input = Input(shape=(max_length,))
  embedding = Embedding(max_words, embedding_dim)(input)
  conv1 = Conv1D(128,2,activation='relu')(embedding)
  pool1 = MaxPooling1D(2)(conv1)
  flat = Flatten()(pool1)
  output_layer = Dense(number_of_classes, activation='softmax')(flat)
  model = Model(inputs=input, outputs=output_layer)

  return model

# call the model
cnn_model = CNN()

# cnn_model.summary()

cnn_model.compile(optimizer = Adam(),
                      loss = 'sparse_categorical_crossentropy',
                      metrics =['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history = cnn_model.fit(train_corpus,
#                               train_data['enc_label'],
#                               epochs = 10,
#                               batch_size = 32,
#                               verbose = 1,
#                               validation_split = 0.1)

y_pred = np.argmax(cnn_model.predict(test_corpus), axis=-1)

print(classification_report(test_data['enc_label'],y_pred))

"""# BiLstm"""

keras.backend.clear_session()

max_length = 100
embedding_dim = 100
number_of_classes = 6


# define bilstm model

def lstm():


   ###### BiLSTM Model #######
  bi_text_inputs = Input(shape=(max_length,))
  bi_embedding_layer = Embedding(max_words,embedding_dim)(bi_text_inputs)
  LSTM_Layer_1 = Bidirectional(LSTM(32))(bi_embedding_layer)
  bi_dense_layer_1 = Dense(number_of_classes, activation='softmax')(LSTM_Layer_1)
  bilstm_model = Model(inputs=bi_text_inputs, outputs=bi_dense_layer_1)


  return bilstm_model

# call the model
lstm_model = lstm()

# lstm_model.summary()

lstm_model.compile(optimizer = Adam(),
                      loss = 'sparse_categorical_crossentropy',
                      metrics =['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history = lstm_model.fit(train_corpus,
#                               train_data['enc_label'],
#                               epochs = 10,
#                               batch_size = 32,
#                               verbose = 1,
#                               validation_split = 0.1)

y_pred = np.argmax(lstm_model.predict(test_corpus), axis=-1)

print(classification_report(test_data['enc_label'],y_pred))

"""# BiLstm + CNN model"""

keras.backend.clear_session()

# define bilstm + Cnn model

def lstm_cnn():


  lc_text_inputs = Input(shape=(max_length,))
  lc_embedding_layer = Embedding(max_words,embedding_dim)(lc_text_inputs)
  LSTM_Layer = Bidirectional(LSTM(32,return_sequences=True))(lc_embedding_layer)
  lc_conv1 = Conv1D(32,2,activation='relu')(LSTM_Layer)
  lc_pool1 = MaxPooling1D(2)(lc_conv1)
  lc_flat = Flatten()(lc_pool1)
  lc_dense_layer_1 = Dense(number_of_classes, activation='softmax')(lc_flat)
  cnn_lstm_model = Model(inputs=lc_text_inputs, outputs=lc_dense_layer_1)


  return cnn_lstm_model

# call the model
lstm_cnn_model = lstm_cnn()

lstm_cnn_model.summary()

lstm_cnn_model.compile(optimizer = 'adam',
                      loss = 'sparse_categorical_crossentropy',
                      metrics =['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history = lstm_cnn_model.fit(train_corpus,
#                               train_data['enc_label'],
#                               epochs = 10,
#                               batch_size = 32,
#                               verbose = 1,
#                               validation_split = 0.1)

y_pred = np.argmax(lstm_cnn_model.predict(test_corpus), axis=-1)

print(classification_report(test_data['enc_label'],y_pred))

"""BILSTM+ATTENtion 01"""

from keras.layers import Layer
from tensorflow.python.keras.layers import Concatenate
import keras.backend as K

max_len = 200
rnn_cell_size = 128

class Attention(tf.keras.Model):
    def __init__(self, units):
        super(Attention, self).__init__()
        self.W1 = tf.keras.layers.Dense(units)
        self.W2 = tf.keras.layers.Dense(units)
        self.V = tf.keras.layers.Dense(1)

    def call(self, features, hidden):
        hidden_with_time_axis = tf.expand_dims(hidden, 1)
        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))
        attention_weights = tf.nn.softmax(self.V(score), axis=1)
        context_vector = attention_weights * features
        context_vector = tf.reduce_sum(context_vector, axis=1)

        return context_vector, attention_weights

sequence_input = Input(shape=(max_len,), dtype='int32')

embedded_sequences = keras.layers.Embedding(max_words, 128, input_length=max_len)(sequence_input)

import os
lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM
                                     (rnn_cell_size,
                                      dropout=0.3,
                                      return_sequences=True,
                                      return_state=True,
                                      recurrent_activation='relu',
                                      recurrent_initializer='glorot_uniform'), name="bi_lstm_0")(embedded_sequences)

lstm, forward_h, forward_c, backward_h, backward_c = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_cell_size,dropout=0.2,return_sequences=True,return_state=True,
      recurrent_activation='relu',
      recurrent_initializer='glorot_uniform'))(lstm)

state_h = Concatenate()([forward_h, backward_h])
state_c = Concatenate()([forward_c, backward_c])

context_vector, attention_weights = attention(lstm, state_h)

output = keras.layers.Dense(6, activation='softmax')(context_vector)

model = keras.Model(inputs=sequence_input, outputs=output)

# summarize layers
print(model.summary())

"""# BiLstm + Attention"""

from keras.layers import Layer
import keras.backend as K

class attention(Layer):
    def __init__(self,**kwargs):
        super(attention,self).__init__(**kwargs)

    def build(self,input_shape):
        self.W=self.add_weight(name="att_weight",shape=(input_shape[-1],1),initializer="normal")
        self.b=self.add_weight(name="att_bias",shape=(input_shape[1],1),initializer="zeros")
        super(attention, self).build(input_shape)

    def call(self,x):
        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)
        at=K.softmax(et)
        at=K.expand_dims(at,axis=-1)
        output=x*at
        return K.sum(output,axis=1)

    def compute_output_shape(self,input_shape):
        return (input_shape[0],input_shape[-1])

    def get_config(self):
        return super(attention,self).get_config()

keras.backend.clear_session()

max_length = 100
embedding_dim = 100
number_of_classes = 6

inputs=Input(shape=(max_length,), name='input')
x=Embedding(max_words,embedding_dim)(inputs)
att_in=Bidirectional(LSTM(256,return_sequences=True,dropout=0.3,recurrent_dropout=0.2))(x)
att_out=attention()(att_in)
outputs=Dense(6,activation='softmax',trainable=True, name="Dense_layer")(att_out)
model=Model(inputs,outputs,name='Text_model')
model.summary()

model.compile(optimizer = Adam(learning_rate=3e-3),
                      loss = 'sparse_categorical_crossentropy',
                      metrics =['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history = model.fit(train_corpus,
#                               train_data['enc_label'],
#                               epochs = 10,
#                               batch_size = 32,
#                               callbacks=checkpoint_fn('bilstmAttention'),
#                               validation_split = 0.1)

# from keras.models import load_model
# saved_model = load_model('/content/drive/MyDrive/New folder/Model/bilstmAttention.h5')

y_pred = np.argmax(model.predict(test_corpus), axis=-1)

print(classification_report(test_data['enc_label'],y_pred))

"""# Bert-Embedding"""

!pip install transformers

############################## Sentence Encoding for Transformers

def bert_encode(data,maximum_length,tokenizer) :
  input_ids = []
  attention_masks = []


  for i in range(len(data.clean_Text)):
      encoded = tokenizer.encode_plus(

        data.clean_Text[i],
        add_special_tokens=True,
        max_length=maximum_length,
        pad_to_max_length=True,
        truncation=True,
        return_attention_mask=True,
        # return_tensors='pt'

      )

      input_ids.append(encoded['input_ids'])
      attention_masks.append(encoded['attention_mask'])
  return np.array(input_ids),np.array(attention_masks)

#######################################             Model Defination

#  output[0:, :, :] ----> word embeddings
#  output[:, 0, :]  ----> Sentence Embedding


def create_model(bert_model,max_len):
  input_ids = Input(shape=(max_len,),dtype='int32')
  attention_masks = Input(shape=(max_len,),dtype='int32')
  output = bert_model([input_ids,attention_masks])
  output = output[0]
  # last_hidden_state, pooler_output = output[0]                            ## 0 for distillbert
  #output = Dense(32,activation='relu')(output[:, 0, :])
  #output = Dropout(0.1)(output)

  output1 = Dense(6,activation='softmax')(output[:, 0, :])
  model = Model(inputs = [input_ids,attention_masks],outputs = output1)
  return model

"""# multilingual-bert"""

from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')

vocab_size = tokenizer.vocab_size

print("Vocabulary size:", vocab_size)

train_input_ids,train_attention_masks = bert_encode(train_data,100,tokenizer)
# valid_input_ids,valid_attention_masks = bert_encode(valid_data,50,tokenizer)
test_input_ids,test_attention_masks = bert_encode(test_data,100,tokenizer)

from transformers import TFBertModel
bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')

with open("vocab.txt", "r", encoding="utf-8") as f:
    vocab_list = f.read().splitlines()

print("Vocabulary size:", len(vocab_list))

### Checkpoint
filepath = '/content/drive/MyDrive/New folder/Model/bert_model.h5'
checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=2, save_best_only=True,
                                             save_weights_only=True, mode='max' )

keras.backend.clear_session()
# 3e-4, 1e-4, 5e-5, 3e-5,1e-5
model = create_model(bert_model,100)
model.compile(Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

model.fit([train_input_ids,train_attention_masks],
                    train_data['enc_label'],
                    validation_split = 0.1,
                    epochs=6,
                    batch_size=16,
                    callbacks = [checkpoint]
                    # class_weight = weight
          )

model = create_model(bert_model,100)
# Load the saved model
model.load_weights('/content/drive/MyDrive/New folder/Model/bert_model.h5')
y_pred = np.argmax(model.predict([test_input_ids,test_attention_masks]), axis=-1)
print(classification_report(test_data["enc_label"], y_pred))

f1_score(test_data["enc_label"], y_pred, average='weighted')

"""## resnet + bert"""

def create_bert_resnet_model ():
  keras.backend.clear_session()
  # create the base pre-trained model
  resnet = ResNet50(weights='imagenet', include_top=False,input_shape=(100, 100, 3))
  for layer in resnet.layers[0:-10]:
      layer.trainable = False
  # add a global spatial average pooling layer
  x = resnet.output
  pool = GlobalAveragePooling2D()(x)
  # let's add a fully-connected layer
  flat = Flatten()(pool)
  # and a logistic layer -- let's say we have 200 classes
  hidden2 = Dense(200, activation='relu')(flat)
  # this is the model we will train
  resnet_img_model = Model(inputs=resnet.input, outputs=hidden2)


  # Text Model
  bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')
  input_ids = Input(shape=(100,),dtype='int32')
  attention_masks = Input(shape=(100,),dtype='int32')
  output = bert_model([input_ids,attention_masks])
  output = output[0]
  # last_hidden_state, pooler_output = output[0]                            ## 0 for distillbert
  #output = Dense(32,activation='relu')(output[:, 0, :])
  #output = Dropout(0.1)(output)

  output1 = Dense(200, activation='relu')(output[:, 0, :])
  bert_model = Model(inputs = [input_ids,attention_masks],outputs = output1)

  # Concatenating the output of 2 classifiers
  con_layer = keras.layers.concatenate([resnet_img_model.output, bert_model.output])
  dropout = Dropout(0.2)(con_layer)
  final_dense = Dense(100, activation="relu")(dropout)
  out = Dense(6,activation='softmax')(final_dense)

  #Defining model input and output
  com_model = Model(inputs = [resnet_img_model.input, bert_model.input], outputs=out)

  return com_model

model = create_bert_resnet_model()

# 3e-4---->  0.76, 1e-4 ---> 0.72, 5e-5 ---> .8987239486293646, 3e-5---> 0.9316320633014693,1e-5---> .9018863525937759

model.compile(loss='sparse_categorical_crossentropy',
                      optimizer=Adam(learning_rate=3e-5),
                      metrics = ["accuracy"])

filepath = f'/content/drive/MyDrive/New folder/Model/bert_resnet.h5'
checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=2, save_best_only=True,
                                             save_weights_only=True, mode='max' )

model.fit([train_image,train_input_ids,train_attention_masks],
          train_data['enc_label'],
          batch_size=16,
          epochs=7,
          validation_split=0.10,
          verbose =1,
          callbacks = [checkpoint] )

model = create_bert_resnet_model()
# Load the saved model
model.load_weights('/content/drive/MyDrive/New folder/Model/bert_resnet.h5')
y_pred = np.argmax(model.predict([test_image,test_input_ids,test_attention_masks]), axis=-1)
print(classification_report(test_data["enc_label"], y_pred))

f1_score(test_data["enc_label"], y_pred, average='weighted')

"""# English-bert"""

from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

train_input_ids,train_attention_masks = bert_encode(train_data,100,tokenizer)
# valid_input_ids,valid_attention_masks = bert_encode(valid_data,50,tokenizer)
test_input_ids,test_attention_masks = bert_encode(test_data,100,tokenizer)

from transformers import TFBertModel
bert_model = TFBertModel.from_pretrained('bert-base-uncased')

### Checkpoint
filepath = '/content/drive/MyDrive/New folder/Model/bert_eng_model.h5'
checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=2, save_best_only=True,
                                             save_weights_only=True, mode='max' )

keras.backend.clear_session()
# 3e-4, 1e-4, 5e-5, 3e-5,1e-5
model = create_model(bert_model,100)
model.compile(Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

model.fit([train_input_ids,train_attention_masks],
                    train_data['enc_label'],
                    validation_split = 0.1,
                    epochs=6,
                    batch_size=16,
                    callbacks = [checkpoint]
                    # class_weight = weight
          )

model = create_model(bert_model,100)
# Load the saved model
model.load_weights('/content/drive/MyDrive/New folder/Model/bert_eng_model.h5')
y_pred = np.argmax(model.predict([test_input_ids,test_attention_masks]), axis=-1)
print(classification_report(test_data["enc_label"], y_pred))

"""## eng_bert + resnet"""

def create_engbert_resnet_model ():
  keras.backend.clear_session()
  # create the base pre-trained model
  resnet = ResNet50(weights='imagenet', include_top=False,input_shape=(100, 100, 3))
  for layer in resnet.layers[0:-10]:
      layer.trainable = False
  # add a global spatial average pooling layer
  x = resnet.output
  pool = GlobalAveragePooling2D()(x)
  # let's add a fully-connected layer
  flat = Flatten()(pool)
  # and a logistic layer -- let's say we have 200 classes
  hidden2 = Dense(200, activation='relu')(flat)
  # this is the model we will train
  resnet_img_model = Model(inputs=resnet.input, outputs=hidden2)


  # Text Model
  bert_model = TFBertModel.from_pretrained('bert-base-uncased')
  input_ids = Input(shape=(100,),dtype='int32')
  attention_masks = Input(shape=(100,),dtype='int32')
  output = bert_model([input_ids,attention_masks])
  output = output[0]
  # last_hidden_state, pooler_output = output[0]                            ## 0 for distillbert
  #output = Dense(32,activation='relu')(output[:, 0, :])
  #output = Dropout(0.1)(output)

  output1 = Dense(200, activation='relu')(output[:, 0, :])
  bert_model = Model(inputs = [input_ids,attention_masks],outputs = output1)

  # Concatenating the output of 2 classifiers
  con_layer = keras.layers.concatenate([resnet_img_model.output, bert_model.output])
  dropout = Dropout(0.2)(con_layer)
  final_dense = Dense(100, activation="relu")(dropout)
  out = Dense(6,activation='softmax')(final_dense)

  #Defining model input and output
  com_model = Model(inputs = [resnet_img_model.input, bert_model.input], outputs=out)

  return com_model

model = create_engbert_resnet_model()

# 3e-4---->  0.76, 1e-4 ---> 0.72, 5e-5 ---> .8987239486293646, 3e-5---> 0.9316320633014693,1e-5---> .9018863525937759

model.compile(loss='sparse_categorical_crossentropy',
                      optimizer=Adam(learning_rate=3e-5),
                      metrics = ["accuracy"])

filepath = f'/content/drive/MyDrive/New folder/Model/engbert_resnet.h5'
checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=2, save_best_only=True,
                                             save_weights_only=True, mode='max' )

model.fit([train_image,train_input_ids,train_attention_masks],
          train_data['enc_label'],
          batch_size=16,
          epochs=7,
          validation_split=0.10,
          verbose =1,
          callbacks = [checkpoint] )

model = create_engbert_resnet_model()
# Load the saved model
model.load_weights('/content/drive/MyDrive/New folder/Model/engbert_resnet.h5')
y_pred = np.argmax(model.predict([test_image,test_input_ids,test_attention_masks]), axis=-1)
print(classification_report(test_data["enc_label"], y_pred))

f1_score(test_data["enc_label"], y_pred, average='weighted')

"""# Tweet-bert"""

from transformers import AutoModel, AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("vinai/bertweet-base", use_fast=False)

train_input_ids,train_attention_masks = bert_encode(train_data,100,tokenizer)
# valid_input_ids,valid_attention_masks = bert_encode(valid_data,50,tokenizer)
test_input_ids,test_attention_masks = bert_encode(test_data,100,tokenizer)

from transformers import TFAutoModel
bertweet = TFAutoModel.from_pretrained("vinai/bertweet-base")
# bertweet = AutoModel.from_pretrained("vinai/bertweet-base")
# # bert_model = TFBertModel.from_pretrained('vinai/bertweet-base')

### Checkpoint
filepath = '/content/drive/MyDrive/New folder/Model/bert_tweet_model.h5'
checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=2, save_best_only=True,
                                             save_weights_only=True, mode='max' )

keras.backend.clear_session()
# 3e-4, 1e-4, 5e-5, 3e-5,1e-5
model = create_model(bertweet,100)
model.compile(Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

model.fit([train_input_ids,train_attention_masks],
                    train_data['enc_label'],
                    validation_split = 0.1,
                    epochs=6,
                    batch_size=16,
                    callbacks = [checkpoint]
                    # class_weight = weight
          )

model = create_model(bertweet,100)
# Load the saved model
model.load_weights('/content/drive/MyDrive/New folder/Model/bert_tweet_model.h5')
y_pred = np.argmax(model.predict([test_input_ids,test_attention_masks]), axis=-1)
print(classification_report(test_data["enc_label"], y_pred))

"""# Multimodal"""

from keras.layers import concatenate
from tensorflow.keras.layers import Concatenate, Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Flatten

def create_lstm_attention_resnet_model ():
  keras.backend.clear_session()
  # create the base pre-trained model
  resnet = ResNet50(weights='imagenet', include_top=False,input_shape=(100, 100, 3))
  for layer in resnet.layers[0:-10]:
      layer.trainable = False
  # add a global spatial average pooling layer
  x = resnet.output
  pool = GlobalAveragePooling2D()(x)
  # let's add a fully-connected layer
  flat = Flatten()(pool)
  # and a logistic layer -- let's say we have 200 classes
  hidden2 = Dense(200, activation='relu')(flat)
  # this is the model we will train
  resnet_img_model = Model(inputs=resnet.input, outputs=hidden2)


  # Text Model

  class Attention(tf.keras.Model):
      def __init__(self, units):
          super(Attention, self).__init__()
          self.W1 = tf.keras.layers.Dense(units)
          self.W2 = tf.keras.layers.Dense(units)
          self.V = tf.keras.layers.Dense(1)

      def call(self, features, hidden):
          # hidden shape == (batch_size, hidden size)
          # hidden_with_time_axis shape == (batch_size, 1, hidden size)
          # we are doing this to perform addition to calculate the score
          hidden_with_time_axis = tf.expand_dims(hidden, 1)

          # score shape == (batch_size, max_length, 1)
          # we get 1 at the last axis because we are applying score to self.V
          # the shape of the tensor before applying self.V is (batch_size, max_length, units)
          score = tf.nn.tanh(
              self.W1(features) + self.W2(hidden_with_time_axis))
          # attention_weights shape == (batch_size, max_length, 1)
          attention_weights = tf.nn.softmax(self.V(score), axis=1)

          # context_vector shape after sum == (batch_size, hidden_size)
          context_vector = attention_weights * features
          context_vector = tf.reduce_sum(context_vector, axis=1)
          return context_vector, attention_weights

  sequence_input = Input(shape=(100,), dtype="int32")
  embedded_sequences = Embedding(vocab_size, 100)(sequence_input)
  lstm = Bidirectional(LSTM(128, return_sequences = True,dropout=0.01), name="bi_lstm_0")(embedded_sequences)
  # Getting our LSTM outputs
  (lstm, forward_h, forward_c, backward_h, backward_c) = Bidirectional(LSTM(128, return_sequences=True, return_state=True), name="bi_lstm_1")(lstm)

  state_h = Concatenate()([forward_h, backward_h])
  state_c = Concatenate()([forward_c, backward_c])
  context_vector, attention_weights = Attention(20)(lstm, state_h)
  dense1 = Dense(200, activation="relu")(context_vector)


  lstm_attention_model = Model(inputs=sequence_input, outputs=dense1)

  # Concatenating the output of 2 classifiers
  con_layer = keras.layers.concatenate([resnet_img_model.output, lstm_attention_model.output])
  dropout = Dropout(0.2)(con_layer)
  final_dense = Dense(100, activation="relu")(dropout)
  out = Dense(6,activation='softmax')(final_dense)

  #Defining model input and output
  com_model = Model(inputs = [resnet_img_model.input, lstm_attention_model.input], outputs=out)

  return com_model

model = create_lstm_attention_resnet_model()

model.compile(loss='sparse_categorical_crossentropy',
                      optimizer=Adam(),
                      metrics = ["accuracy"])

filepath = f'/content/drive/MyDrive/New folder/Model/Attn_multi.h5'
checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=2, save_best_only=True,
                                             save_weights_only=True, mode='max' )

model.fit([train_image,train_corpus],
          train_data['enc_label'],
          batch_size=32,
          epochs=50,
          validation_split=0.10,
          verbose =1,
          callbacks = [checkpoint] )

model = create_lstm_attention_resnet_model()
# Load the saved model
model.load_weights('/content/drive/MyDrive/New folder/Model/Attn_multi.h5')
y_pred = np.argmax(model.predict([test_image,test_corpus]), axis=-1)
print(classification_report(test_data["enc_label"], y_pred))

f1_score(test_data["enc_label"], y_pred, average='weighted')



"""# Inception + BilstmAttention"""

keras.backend.clear_session()

max_length = 100
embedding_dim = 100
number_of_classes = 6

inputs=Input(shape=(max_length,), name='input')
x=Embedding(max_words,embedding_dim)(inputs)
att_in=Bidirectional(LSTM(32,return_sequences=True,dropout=0.3,recurrent_dropout=0.2))(x)
att_out=attention()(att_in)
branch_1 = Dense(512, activation='relu')(att_out)

# Image input branch - a pre-trained Inception module followed by an added fully connected layer
base_model = InceptionV3(weights='imagenet', include_top=False)
# Freeze Inception's weights - we don't want to train these
for layer in base_model.layers:
    layer.trainable = False

# add a fully connected layer after Inception - we do want to train these
branch_2 = base_model.output
branch_2 = GlobalAveragePooling2D()(branch_2)
branch_2 = Dense(1024, activation='relu')(branch_2)

# merge the text input branch and the image input branch and add another fully connected layer
joint = concatenate([branch_1, branch_2])
joint = Dense(512, activation='relu')(joint)
joint = Dropout(0.5)(joint)
predictions = Dense(6, activation='softmax')(joint)

full_model = Model(inputs=[base_model.input,inputs], outputs=[predictions])

full_model.compile(loss='sparse_categorical_crossentropy',
                   optimizer='rmsprop',
                   metrics=['accuracy'])

history = full_model.fit([train_image, train_corpus], train_data['enc_label'],
                         epochs=10, batch_size=32,
                         verbose=1, validation_split=0.1, shuffle=True)

y_pred = np.argmax(model.predict(test_corpus), axis=-1)

print(classification_report(test_data['enc_label'],y_pred))

f1_score(test_data["enc_label"], y_pred, average='weighted')

"""# Resnet50 + BilstmAttention"""

keras.backend.clear_session()

max_length = 100
embedding_dim = 100
number_of_classes = 6

inputs=Input(shape=(max_length,), name='input')
x=Embedding(max_words,embedding_dim)(inputs)
att_in=Bidirectional(LSTM(256,return_sequences=True,dropout=0.3,recurrent_dropout=0.2))(x)
att_out=attention()(att_in)
branch_1 = Dense(256, activation='relu')(att_out)

# Image input branch - a pre-trained Inception module followed by an added fully connected layer
base_model = ResNet50(weights='imagenet', include_top=False)
# Freeze Inception's weights - we don't want to train these
for layer in base_model.layers:
    layer.trainable = False

# add a fully connected layer after Inception - we do want to train these
branch_2 = base_model.output
branch_2 = GlobalAveragePooling2D()(branch_2)
branch_2 = Dense(512, activation='relu')(branch_2)

# merge the text input branch and the image input branch and add another fully connected layer
joint = concatenate([branch_1, branch_2])
joint = Dense(100, activation='relu')(joint)
# joint = Dropout(0.5)(joint)
predictions = Dense(6, activation='softmax')(joint)

full_model = Model(inputs=[base_model.input,inputs], outputs=[predictions])

full_model.compile(loss='sparse_categorical_crossentropy',
                   optimizer=Adam(),
                   metrics=['accuracy'])

history = full_model.fit([train_image, train_corpus], train_data['enc_label'],
                         epochs=10, batch_size=32,
                         verbose=1, validation_split=0.1, callbacks=checkpoint_fn('ResBiLstmAttention'),shuffle=True)

y_pred = np.argmax(full_model.predict([test_image,test_corpus]), axis=-1)

print(classification_report(test_data["enc_label"], y_pred))

f1_score(test_data["enc_label"], y_pred, average='weighted')

